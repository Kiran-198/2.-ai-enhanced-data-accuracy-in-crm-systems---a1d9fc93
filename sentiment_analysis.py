# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Podbwiby8aNZqZjY1PebUBgJRMdDoerx
"""

import numpy as np
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/college/sem7/FYP/tweets_dataset - labelled.csv')
df.head()

df1 = df.drop(['user_name', 'user_location', 'user_description', 'user_verified', 'date', 'hashtags', 'source'], axis=1)
df1 = df1[df1['label'] != 'neutral']
df1 = df1.replace(np.nan, '', regex=True)

df1.head()

df1['label'].value_counts()

import nltk
nltk.download('stopwords')

import re
import pandas as pd
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
import spacy

nlp = spacy.load('en', disable=['parser', 'ner'])

df1['text'] = df1['text'].apply(lambda x: " ".join(x.lower() for x in x.split()))
df1['text'] = df1['text'].str.replace('[^\w\s]','')
stop = stopwords.words('english')
df1['text'] = df1['text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

def space(comment):
    doc = nlp(comment)
    return " ".join([token.lemma_ for token in doc])

df1['text']= df1['text'].apply(space)

df1.head()

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)
text_counts = cv.fit_transform(df1['text'])

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df1['label'], test_size=0.25, random_state=5)

from sklearn.naive_bayes import MultinomialNB
MNB = MultinomialNB()
MNB.fit(X_train, Y_train)

predicted = MNB.predict(X_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(confusion_matrix(Y_test,predicted))
print(classification_report(Y_test,predicted))
print("Accuracy: ", accuracy_score(Y_test, predicted))



from sklearn.linear_model import SGDClassifier
SGDC = SGDClassifier()
SGDC.fit(X_train, Y_train)

predictions = SGDC.predict(X_test)

print(confusion_matrix(Y_test,predictions))
print(classification_report(Y_test,predictions))
print("Accuracy: ", accuracy_score(Y_test, predictions))



from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier(criterion='entropy', random_state=1)
DT.fit(X_train, Y_train)

prediction = DT.predict(X_test)

print(confusion_matrix(Y_test,prediction))
print(classification_report(Y_test,prediction))
print("Accuracy: ", accuracy_score(Y_test, prediction))



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df1.text, df1.label, test_size=0.2, random_state=32)

from sklearn.feature_extraction.text import TfidfVectorizer
# Create feature vectors
vectorizer = TfidfVectorizer(min_df = 5,
                             max_df = 0.8,
                             sublinear_tf = True,
                             use_idf = True)
train_vectors = vectorizer.fit_transform(X_train)
test_vectors = vectorizer.transform(X_test)

from sklearn import svm
from sklearn.metrics import classification_report
classifier_linear = svm.SVC(kernel='linear')
classifier_linear.fit(train_vectors, y_train)
prediction_linear = classifier_linear.predict(test_vectors)

print(classification_report(y_test,prediction_linear))

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
vectorizer.fit(X_train)

X_train = vectorizer.transform(X_train)
X_test= vectorizer.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
classifier = LogisticRegression(max_iter=1000)
classifier.fit(X_train, y_train)

predictions = classifier.predict(X_test)

print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))
print("Accuracy: ", accuracy_score(y_test, predictions))